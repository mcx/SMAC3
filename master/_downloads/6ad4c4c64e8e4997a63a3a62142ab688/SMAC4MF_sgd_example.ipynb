{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Optimizing average cross-validation performance with SMAC4MF\n\n\nAn example for the usage of Hyperband intensifier in SMAC with multiple instances.\nWe optimize a SGD classifier on the digits dataset as multiple binary classification problems\nusing \"Hyperband\" intensification. We split the digits dataset (10 classes) into 45 binary datasets.\n\nIn this example, we use instances as the budget in hyperband and optimize the average cross\nvalidation accuracy. An \"Instance\" represents a specific scenario/condition (eg: different datasets,\nsubsets, transformations) for the algorithm to run. SMAC then returns the algorithm that had the\nbest performance across all the instances. In this case, an instance is a binary dataset i.e.,\ndigit-2 vs digit-3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nlogging.basicConfig(level=logging.INFO)\n\nimport itertools\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter, UniformFloatHyperparameter\nfrom sklearn import datasets\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\n# Import ConfigSpace and different types of parameters\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_mf_facade import SMAC4MF\n# Import SMAC-utilities\nfrom smac.scenario.scenario import Scenario\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\n# We load the MNIST-dataset (a widely used benchmark) and split it into a list of binary datasets\ndigits = datasets.load_digits()\ninstances = [[str(a) + str(b)] for a, b in itertools.combinations(digits.target_names, 2)]\n\n\ndef generate_instances(a: int, b: int):\n    \"\"\"\n    Function to select data for binary classification from the digits dataset\n    a & b are the two classes\n    \"\"\"\n    # get indices of both classes\n    indices = np.where(np.logical_or(a == digits.target, b == digits.target))\n    # get data\n    data = digits.data[indices]\n    target = digits.target[indices]\n    return data, target\n\n\n# Target Algorithm\ndef sgd_from_cfg(cfg, seed, instance):\n    \"\"\" Creates a SGD classifier based on a configuration and evaluates it on the\n    digits dataset using cross-validation.\n\n    Parameters:\n    -----------\n    cfg: Configuration (ConfigSpace.ConfigurationSpace.Configuration)\n        Configuration containing the parameters.\n        Configurations are indexable!\n    seed: int or RandomState\n        used to initialize the svm's random generator\n    instance: str\n        used to represent the instance to use (the 2 classes to consider in this case)\n\n    Returns:\n    --------\n    float\n        A crossvalidated mean score for the SGD classifier on the loaded data-set.\n    \"\"\"\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=ConvergenceWarning)\n\n        # SGD classifier using given configuration\n        clf = SGDClassifier(loss='log', penalty='elasticnet', alpha=cfg['alpha'],\n                            l1_ratio=cfg['l1_ratio'], learning_rate=cfg['learning_rate'],\n                            eta0=cfg['eta0'], max_iter=30, early_stopping=True, random_state=seed)\n\n        # get instance\n        data, target = generate_instances(int(instance[0]), int(instance[1]))\n\n        cv = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)  # to make CV splits consistent\n        scores = cross_val_score(clf, data, target, cv=cv)\n    return 1 - np.mean(scores)  # Minimize!\n\n\nif __name__ == \"__main__\":\n    # Build Configuration Space which defines all parameters and their ranges\n    cs = ConfigurationSpace()\n\n    # We define a few possible parameters for the SGD classifier\n    alpha = UniformFloatHyperparameter(\"alpha\", 0, 1, default_value=1.0)\n    l1_ratio = UniformFloatHyperparameter(\"l1_ratio\", 0, 1, default_value=0.5)\n    learning_rate = CategoricalHyperparameter(\"learning_rate\", choices=['constant', 'invscaling', 'adaptive'],\n                                              default_value='constant')\n    eta0 = UniformFloatHyperparameter(\"eta0\", 0.00001, 1, default_value=0.1, log=True)\n    # Add the parameters to configuration space\n    cs.add_hyperparameters([alpha, l1_ratio, learning_rate, eta0])\n\n    # SMAC scenario object\n    scenario = Scenario({\"run_obj\": \"quality\",  # we optimize quality (alternative to runtime)\n                        \"wallclock-limit\": 100,  # max duration to run the optimization (in seconds)\n                         \"cs\": cs,  # configuration space\n                         \"deterministic\": True,\n                         \"limit_resources\": True,  # Uses pynisher to limit memory and runtime\n                         \"memory_limit\": 3072,  # adapt this to reasonable value for your hardware\n                         \"cutoff\": 3,  # runtime limit for the target algorithm\n                         \"instances\": instances  # Optimize across all given instances\n                         })\n\n    # intensifier parameters\n    # if no argument provided for budgets, hyperband decides them based on the number of instances available\n    intensifier_kwargs = {'initial_budget': 1, 'max_budget': 45, 'eta': 3,\n                          # You can also shuffle the order of using instances by this parameter.\n                          'instance_order': None,\n                          # 'shuffle' will shuffle instances before each SH run and 'shuffle_once'\n                          # will shuffle instances once before the 1st SH iteration begins\n                          }\n\n    # To optimize, we pass the function to the SMAC-object\n    smac = SMAC4MF(scenario=scenario, rng=np.random.RandomState(42),\n                   tae_runner=sgd_from_cfg,\n                   intensifier_kwargs=intensifier_kwargs\n                   # all arguments related to intensifier can be passed like this\n                   )\n\n    # Example call of the function\n    # It returns: Status, Cost, Runtime, Additional Infos\n    def_costs = []\n    for i in instances:\n        cost = smac.get_tae_runner().run(cs.get_default_configuration(), i[0])[1]\n        def_costs.append(cost)\n    print(\"Value for default configuration: %.4f\" % (np.mean(def_costs)))\n\n    # Start optimization\n    try:\n        incumbent = smac.optimize()\n    finally:\n        incumbent = smac.solver.incumbent\n\n    inc_costs = []\n    for i in instances:\n        cost = smac.get_tae_runner().run(incumbent, i[0])[1]\n        inc_costs.append(cost)\n    print(\"Optimized Value: %.4f\" % (np.mean(inc_costs)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}