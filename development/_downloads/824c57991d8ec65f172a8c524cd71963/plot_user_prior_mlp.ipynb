{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# HPO with User Priors over the Optimum\n\nExample for optimizing a Multi-Layer Perceptron (MLP) setting priors over the optimum on the\nhyperparameters. These priors are derived from user knowledge - from previous runs on similar\ntasks, common knowledge or intuition gained from manual tuning. To create the priors, we make\nuse of the Normal and Beta Hyperparameters, as well as the \"weights\" property of the\nCategoricalHyperparameter. This can be integrated into the optimiztion for any SMAC facade,\nbut we stick with SMAC4HPO here. To incorporate user priors into the optimization, \n\u03c0BO (nolinkexistsyet) is used to bias the point selection strategy.\n\nMLP is used as the deep neural network.\nThe digits datasetis chosen to optimize the average accuracy on 5-fold cross validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nimport warnings\n\nimport ConfigSpace as CS\nimport numpy as np\nfrom ConfigSpace.hyperparameters import (\n    BetaIntegerHyperparameter,\n    CategoricalHyperparameter,\n    NormalFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_bb_facade import SMAC4BB\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\nfrom smac.initial_design.random_configuration_design import RandomConfigurations\nfrom smac.scenario.scenario import Scenario\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\n# Target Algorithm\ndef mlp_from_cfg(cfg, seed):\n    \"\"\"\n    Creates a MLP classifier from sklearn and fits the given data on it.\n\n    Parameters\n    ----------\n    cfg: Configuration\n        configuration chosen by smac\n    seed: int or RandomState\n        used to initialize the rf's random generator\n    budget: float\n        used to set max iterations for the MLP\n\n    Returns\n    -------\n    float\n    \"\"\"\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n        mlp = MLPClassifier(\n            hidden_layer_sizes=[cfg[\"n_neurons\"]] * cfg[\"n_layer\"],\n            solver=cfg[\"optimizer\"],\n            batch_size=cfg[\"batch_size\"],\n            activation=cfg[\"activation\"],\n            learning_rate_init=cfg[\"learning_rate_init\"],\n            random_state=seed,\n        )\n\n        # returns the cross validation accuracy\n        cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n        score = cross_val_score(mlp, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n    return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    # Build Configuration Space which defines all parameters and their ranges.\n    # To illustrate different parameter types,\n    # we use continuous, integer and categorical parameters.\n    cs = ConfigurationSpace()\n\n    # We do not have an educated belief on the number of layers beforehand\n    # As such, the prior on the HP is uniform\n    n_layer = UniformIntegerHyperparameter(\"n_layer\", lower=1, upper=5)\n\n    # We believe the optimal network is likely going to be relatively wide,\n    # And place a Beta Prior skewed towards wider networks in log space\n    n_neurons = BetaIntegerHyperparameter(\"n_neurons\", lower=8, upper=1024, alpha=4, beta=2, log=True)\n\n    # We believe that ReLU is likely going to be the optimal activation function about\n    # 60% of the time, and thus place weight on that accordingly\n    activation = CategoricalHyperparameter(\n        \"activation\", [\"logistic\", \"tanh\", \"relu\"], weights=[1, 1, 3], default_value=\"relu\"\n    )\n\n    # Moreover, we believe ADAM is the most likely optimizer\n    optimizer = CategoricalHyperparameter(\"optimizer\", [\"sgd\", \"adam\"], weights=[1, 2], default_value=\"adam\")\n\n    # We do not have an educated opinion on the batch size, and thus leave it as-is\n    batch_size = UniformIntegerHyperparameter(\"batch_size\", 16, 512, default_value=128)\n\n    # We place a log-normal prior on the learning rate, so that it is centered on 10^-3,\n    # with one unit of standard deviation per multiple of 10 (in log space)\n    learning_rate_init = NormalFloatHyperparameter(\n        \"learning_rate_init\", lower=1e-5, upper=1.0, mu=np.log(1e-3), sigma=np.log(10), log=True\n    )\n\n    # Add all hyperparameters at once:\n    cs.add_hyperparameters([n_layer, n_neurons, activation, optimizer, batch_size, learning_rate_init])\n\n    # SMAC scenario object\n    scenario = Scenario(\n        {\n            \"run_obj\": \"quality\",  # we optimize quality (alternative to runtime)\n            \"runcount-limit\": 20,  # max duration to run the optimization (in seconds)\n            \"cs\": cs,  # configuration space\n            \"deterministic\": \"true\",\n            \"limit_resources\": True,  # Uses pynisher to limit memory and runtime\n            # Alternatively, you can also disable this.\n            # Then you should handle runtime and memory yourself in the TA\n            \"cutoff\": 30,  # runtime limit for target algorithm\n            \"memory_limit\": 3072,  # adapt this to reasonable value for your hardware\n        }\n    )\n\n    # The rate at which SMAC forgets the prior.\n    # The higher the value, the more the prior is considered.\n    # Defaults to # n_iterations / 10\n    user_prior_kwargs = {\"decay_beta\": 1.5}\n\n    # To optimize, we pass the function to the SMAC-object\n    smac = SMAC4HPO(\n        scenario=scenario,\n        rng=np.random.RandomState(42),\n        tae_runner=mlp_from_cfg,\n        # This flag is required to conduct the optimisation using priors over the optimum\n        user_priors=True,\n        user_prior_kwargs=user_prior_kwargs,\n        # Using random configurations will cause the initialization to be samples drawn from the prior\n        initial_design=RandomConfigurations,\n    )\n\n    # Example call of the function with default values\n    # It returns: Status, Cost, Runtime, Additional Infos\n    def_value = smac.get_tae_runner().run(config=cs.get_default_configuration(), seed=0)[1]\n\n    print(\"Value for default configuration: %.4f\" % def_value)\n\n    # Start optimization\n    try:\n        incumbent = smac.optimize()\n    finally:\n        incumbent = smac.solver.incumbent\n\n    inc_value = smac.get_tae_runner().run(config=incumbent, seed=0)[1]\n\n    print(\"Optimized Value: %.4f\" % inc_value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}