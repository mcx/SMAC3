{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Non-Deterministic Gradient-Boosting\n\nWe optimize a GradientBoosting on an artificially created binary classification dataset.\nThe results are not deterministic so we need to evaluate each configuration\nmultiple times. To ensure fair comparison, SMAC will only sample from a fixed set of random seeds and apply them to\ncontrol the randomness of the function to be evaluated.\n\nTo evaluate undeterministic functions, we need to set \"deterministic\" as \"false\".\nAdditional to the configuration, the function should make use of the seed parameter as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nimport numpy as np\nfrom ConfigSpace.hyperparameters import (\n    UniformFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\nfrom smac.scenario.scenario import Scenario\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\n# load data and split it into training and test dataset\nX, y = make_hastie_10_2(random_state=0)\nX_train, X_test = X[:8400], X[8400:]\ny_train, y_test = y[:8400], y[8400:]\n\n\n# Gradient Boosting scored with cross validation\ndef xgboost_from_cfg(cfg, seed=0):\n    # use random seed to control the randomness of the model and cross validator\n    clf = GradientBoostingClassifier(**cfg, random_state=seed).fit(X_train, y_train)\n    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n    scores = cross_val_score(clf, X_train, y_train, cv=cv)\n\n    return 1 - np.mean(scores)\n\n\ndef eval_undeterministic_model(cfg, seeds):\n    # Evaluate an undeterminstic model with the given configuration and a seed pool\n    cfg_cv_scores = [0.0] * len(run_seeds)\n    cfg_test_scores = [0.0] * len(run_seeds)\n    for i, seed in enumerate(seeds):\n        cfg_cv_scores[i] = xgboost_from_cfg(cfg, seed=seed)\n        clf = GradientBoostingClassifier(**cfg, random_state=seed).fit(X_train, y_train)\n        cfg_test_scores[i] = 1 - clf.score(X_test, y_test)\n    return cfg_cv_scores, cfg_test_scores\n\n\nif __name__ == \"__main__\":\n    # creating a Configuration Space with every parameter over which SMAC is going to optimize\n    cs = ConfigurationSpace()\n\n    max_depth = UniformIntegerHyperparameter(\"max_depth\", 1, 10, default_value=3)\n    cs.add_hyperparameter(max_depth)\n\n    learning_rate = UniformFloatHyperparameter(\"learning_rate\", 0.01, 1.0, default_value=1.0, log=True)\n    cs.add_hyperparameter(learning_rate)\n\n    min_samples_split = UniformFloatHyperparameter(\"min_samples_split\", 0.01, 1.0, default_value=0.1, log=True)\n    max_features = UniformIntegerHyperparameter(\"max_features\", 2, 10, default_value=4)\n    cs.add_hyperparameters([min_samples_split, max_features])\n\n    subsample = UniformFloatHyperparameter(\"subsample\", 0.5, 1, default_value=0.8)\n    cs.add_hyperparameter(subsample)\n\n    cfg = cs.get_default_configuration()\n    clf = GradientBoostingClassifier(**cfg, random_state=0).fit(X_train, y_train)\n    def_test_score = 1 - clf.score(X_test, y_test)\n\n    print(\"Default cross validation score: %.2f\" % (xgboost_from_cfg(cfg)))\n    print(\"Default test score: %.2f\" % def_test_score)\n\n    # scenario object\n    scenario = Scenario(\n        {\n            \"run_obj\": \"quality\",\n            \"runcount-limit\": 100,\n            \"cs\": cs,\n            # the evaluations are not deterministic, we need to repeat each\n            # configuration several times and take the mean value of these repetitions\n            \"deterministic\": \"false\",\n            \"wallclock_limit\": 120,\n            \"maxR\": 3,  # Each configuration will be evaluated maximal 3 times with various seeds\n            \"minR\": 1,  # Each configuration will be repeated at least 1 time with different seeds\n        }\n    )\n\n    intensifier_kwargs = {\n        \"maxR\": 3,  # Each configuration will be evaluated maximal 3 times with various seeds\n        \"minR\": 1,  # Each configuration will be repeated at least 1 time with different seeds\n    }\n\n    smac = SMAC4HPO(\n        scenario=scenario,\n        rng=np.random.RandomState(0),\n        intensifier_kwargs=intensifier_kwargs,\n        tae_runner=xgboost_from_cfg,\n    )\n\n    incumbent = smac.optimize()\n\n    # get all the seeds applied to incumbent\n    run_seeds = []\n    for inst_seed_budget in smac.get_runhistory().get_runs_for_config(incumbent, only_max_observed_budget=True):\n        run_seeds.append(inst_seed_budget.seed)\n\n    cfg_default = cs.get_default_configuration()\n\n    cfg_default_cv_scores, cfg_default_test_scores = eval_undeterministic_model(cfg_default, seeds=run_seeds)\n\n    print(\"Default cross validation score: %.2f\" % (np.mean(cfg_default_cv_scores)))\n    print(\"Default test score: %.2f\" % np.mean(cfg_default_test_scores))\n\n    # the optimization process is called\n    cfg_inc_cv_scores, cfg_inc_test_scores = eval_undeterministic_model(cfg_default, seeds=run_seeds)\n    # a classifier is trained with the hyperparameters returned from the optimizer\n    print(\"Score on test set: %.2f\" % np.mean(cfg_inc_test_scores))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}