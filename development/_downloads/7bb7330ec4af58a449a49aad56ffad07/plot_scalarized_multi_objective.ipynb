{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scalarized Multi-Objective Using ParEGO\n\nThis example builds on `SVM with Cross-Validation`.\n\nOptimize both the final performance and the time used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nfrom smac.optimizer.multi_objective.parego import ParEGO\n\nlogging.basicConfig(level=logging.INFO)\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ConfigSpace.conditions import InCondition\nfrom ConfigSpace.hyperparameters import (\n    CategoricalHyperparameter,\n    UniformFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import cross_val_score\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\nfrom smac.scenario.scenario import Scenario\nfrom smac.utils.constants import MAXINT\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n# We load the iris-dataset (a widely used benchmark)\niris = datasets.load_iris()\n\n\ndef is_pareto_efficient_simple(costs):\n    \"\"\"\n    Plot the Pareto Front in our 2d example.\n\n    source from: https://stackoverflow.com/a/40239615\n    Find the pareto-efficient points\n    :param costs: An (n_points, n_costs) array\n    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient\n    \"\"\"\n\n    is_efficient = np.ones(costs.shape[0], dtype=bool)\n    for i, c in enumerate(costs):\n        if is_efficient[i]:\n            # Keep any point with a lower cost\n            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n\n            # And keep self\n            is_efficient[i] = True\n    return is_efficient\n\n\ndef plot_pareto_from_runhistory(observations):\n    \"\"\"\n    This is only an example function for 2d plotting, when both objectives\n    are to be minimized\n    \"\"\"\n\n    # find the pareto front\n    efficient_mask = is_pareto_efficient_simple(observations)\n    front = observations[efficient_mask]\n    # observations = observations[np.invert(efficient_mask)]\n\n    obs1, obs2 = observations[:, 0], observations[:, 1]\n    front = front[front[:, 0].argsort()]\n\n    # add the bounds\n    x_upper = np.max(obs1)\n    y_upper = np.max(obs2)\n    front = np.vstack([[front[0][0], y_upper], front, [x_upper, np.min(front[:, 1])]])\n\n    x_front, y_front = front[:, 0], front[:, 1]\n\n    plt.scatter(obs1, obs2)\n    plt.step(x_front, y_front, where=\"post\", linestyle=\":\")\n    plt.title(\"Pareto-Front\")\n\n    plt.xlabel(\"Cost\")\n    plt.ylabel(\"Time\")\n    plt.show()\n\n\ndef svm_from_cfg(cfg):\n    \"\"\"Creates a SVM based on a configuration and evaluates it on the\n    iris-dataset using cross-validation. Note here random seed is fixed.\n\n    It is a multi-objective tae, because we wish to trade-off the time to train\n    and the algorithm's final performance.\n\n    Parameters:\n    -----------\n    cfg: Configuration (ConfigSpace.ConfigurationSpace.Configuration)\n        Configuration containing the parameters.\n        Configurations are indexable!\n\n    Returns:\n    --------\n    Dict: A crossvalidated mean score (cost) for the svm on the loaded data-set and the\n    second objective; runtime\n    \"\"\"\n\n    # For deactivated parameters, the configuration stores None-values.\n    # This is not accepted by the SVM, so we remove them.\n    cfg = {k: cfg[k] for k in cfg if cfg[k]}\n    # And for gamma, we set it to a fixed value or to \"auto\" (if used)\n    if \"gamma\" in cfg:\n        cfg[\"gamma\"] = cfg[\"gamma_value\"] if cfg[\"gamma\"] == \"value\" else \"auto\"\n        cfg.pop(\"gamma_value\", None)  # Remove \"gamma_value\"\n\n    t0 = time.time()\n    clf = svm.SVC(**cfg, random_state=42)\n    t1 = time.time()\n\n    scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n    cost_value = 1 - np.mean(scores)  # Minimize!\n\n    # Return a dictionary with all of the objectives.\n    # Alternatively you can return a list in the same order\n    # as `multi_objectives`.\n    return {\"cost\": cost_value, \"time\": t1 - t0}\n\n\nif __name__ == \"__main__\":\n    # Build Configuration Space which defines all parameters and their ranges\n    cs = ConfigurationSpace()\n\n    # We define a few possible types of SVM-kernels and add them as \"kernel\" to our cs\n    kernel = CategoricalHyperparameter(\n        name=\"kernel\",\n        choices=[\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n        default_value=\"poly\",\n    )\n    cs.add_hyperparameter(kernel)\n\n    # There are some hyperparameters shared by all kernels\n    C = UniformFloatHyperparameter(\"C\", 0.001, 1000.0, default_value=1.0, log=True)\n    shrinking = CategoricalHyperparameter(\"shrinking\", [True, False], default_value=True)\n    cs.add_hyperparameters([C, shrinking])\n\n    # Others are kernel-specific, so we can add conditions to limit the searchspace\n    degree = UniformIntegerHyperparameter(\"degree\", 1, 5, default_value=3)  # Only used by kernel poly\n    coef0 = UniformFloatHyperparameter(\"coef0\", 0.0, 10.0, default_value=0.0)  # poly, sigmoid\n    cs.add_hyperparameters([degree, coef0])\n\n    use_degree = InCondition(child=degree, parent=kernel, values=[\"poly\"])\n    use_coef0 = InCondition(child=coef0, parent=kernel, values=[\"poly\", \"sigmoid\"])\n    cs.add_conditions([use_degree, use_coef0])\n\n    # This also works for parameters that are a mix of categorical and values\n    # from a range of numbers\n    # For example, gamma can be either \"auto\" or a fixed float\n    gamma = CategoricalHyperparameter(\"gamma\", [\"auto\", \"value\"], default_value=\"auto\")  # only rbf, poly, sigmoid\n    gamma_value = UniformFloatHyperparameter(\"gamma_value\", 0.0001, 8, default_value=1, log=True)\n    cs.add_hyperparameters([gamma, gamma_value])\n    # We only activate gamma_value if gamma is set to \"value\"\n    cs.add_condition(InCondition(child=gamma_value, parent=gamma, values=[\"value\"]))\n    # And again we can restrict the use of gamma in general to the choice of the kernel\n    cs.add_condition(InCondition(child=gamma, parent=kernel, values=[\"rbf\", \"poly\", \"sigmoid\"]))\n\n    # Scenario object\n    scenario = Scenario(\n        {\n            \"run_obj\": \"quality\",  # we optimize quality (alternatively runtime)\n            \"runcount-limit\": 50,  # max. number of function evaluations\n            \"cs\": cs,  # configuration space\n            \"deterministic\": True,\n            \"multi_objectives\": [\"cost\", \"time\"],\n            # You can define individual crash costs for each objective\n            \"cost_for_crash\": [1, float(MAXINT)],\n        }\n    )\n\n    # Example call of the function\n    # It returns: Status, Cost, Runtime, Additional Infos\n    def_value = svm_from_cfg(cs.get_default_configuration())\n    print(\"Default config's cost: {cost:2f}, training time: {time:2f} seconds\".format(**def_value))\n\n    # Optimize, using a SMAC-object\n    print(\"Optimizing! Depending on your machine, this might take a few minutes.\")\n    # Pass the multi objective algorithm and its hyperparameters\n    smac = SMAC4HPO(\n        scenario=scenario,\n        rng=np.random.RandomState(42),\n        tae_runner=svm_from_cfg,\n        multi_objective_algorithm=ParEGO,\n        multi_objective_kwargs={\n            \"rho\": 0.05,\n        },\n    )\n\n    incumbent = smac.optimize()\n\n    # pareto front based on smac.runhistory.data\n    cost = np.vstack([v[0] for v in smac.runhistory.data.values()])\n    plot_pareto_from_runhistory(cost)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}