{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MLP with Multi-Fidelity\n\nExample for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets.\nSince we want to take advantage of Multi-Fidelity, the SMAC4MF facade is a good choice. By default,\nSMAC4MF internally runs with `hyperband <https://arxiv.org/abs/1603.06560>`_, which is a combination of an\naggressive racing mechanism and successive halving.\n\nMLP is a deep neural network, and therefore, we choose epochs as fidelity type. The digits dataset\nis chosen to optimize the average accuracy on 5-fold cross validation.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example uses the ``SMAC4MF`` facade, which is the closest implementation to\n    `BOHB <https://github.com/automl/HpBandSter>`_.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nimport warnings\n\nimport ConfigSpace as CS\nimport numpy as np\nfrom ConfigSpace.hyperparameters import (\n    CategoricalHyperparameter,\n    UniformFloatHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_mf_facade import SMAC4MF\nfrom smac.scenario.scenario import Scenario\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\n# Target Algorithm\ndef mlp_from_cfg(cfg, seed, budget):\n    \"\"\"\n    Creates a MLP classifier from sklearn and fits the given data on it.\n\n    Parameters\n    ----------\n    cfg: Configuration\n        configuration chosen by smac\n    seed: int or RandomState\n        used to initialize the rf's random generator\n    budget: float\n        used to set max iterations for the MLP\n\n    Returns\n    -------\n    float\n    \"\"\"\n\n    # For deactivated parameters, the configuration stores None-values.\n    # This is not accepted by the MLP, so we replace them with placeholder values.\n    lr = cfg[\"learning_rate\"] if cfg[\"learning_rate\"] else \"constant\"\n    lr_init = cfg[\"learning_rate_init\"] if cfg[\"learning_rate_init\"] else 0.001\n    batch_size = cfg[\"batch_size\"] if cfg[\"batch_size\"] else 200\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n        mlp = MLPClassifier(\n            hidden_layer_sizes=[cfg[\"n_neurons\"]] * cfg[\"n_layer\"],\n            solver=cfg[\"solver\"],\n            batch_size=batch_size,\n            activation=cfg[\"activation\"],\n            learning_rate=lr,\n            learning_rate_init=lr_init,\n            max_iter=int(np.ceil(budget)),\n            random_state=seed,\n        )\n\n        # returns the cross validation accuracy\n        cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n        score = cross_val_score(mlp, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n    return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    # Build Configuration Space which defines all parameters and their ranges.\n    # To illustrate different parameter types,\n    # we use continuous, integer and categorical parameters.\n    cs = ConfigurationSpace()\n\n    n_layer = UniformIntegerHyperparameter(\"n_layer\", 1, 5, default_value=1)\n    n_neurons = UniformIntegerHyperparameter(\"n_neurons\", 8, 1024, log=True, default_value=10)\n    activation = CategoricalHyperparameter(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default_value=\"tanh\")\n    solver = CategoricalHyperparameter(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default_value=\"adam\")\n    batch_size = UniformIntegerHyperparameter(\"batch_size\", 30, 300, default_value=200)\n    learning_rate = CategoricalHyperparameter(\n        \"learning_rate\",\n        [\"constant\", \"invscaling\", \"adaptive\"],\n        default_value=\"constant\",\n    )\n    learning_rate_init = UniformFloatHyperparameter(\"learning_rate_init\", 0.0001, 1.0, default_value=0.001, log=True)\n\n    # Add all hyperparameters at once:\n    cs.add_hyperparameters(\n        [\n            n_layer,\n            n_neurons,\n            activation,\n            solver,\n            batch_size,\n            learning_rate,\n            learning_rate_init,\n        ]\n    )\n\n    # Adding conditions to restrict the hyperparameter space\n    # Since learning rate is used when solver is 'sgd'\n    use_lr = CS.conditions.EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n    # Since learning rate initialization will only be accounted for when using 'sgd' or 'adam'\n    use_lr_init = CS.conditions.InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n    # Since batch size will not be considered when optimizer is 'lbfgs'\n    use_batch_size = CS.conditions.InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n    # We can also add  multiple conditions on hyperparameters at once:\n    cs.add_conditions([use_lr, use_batch_size, use_lr_init])\n\n    # SMAC scenario object\n    scenario = Scenario(\n        {\n            \"run_obj\": \"quality\",  # we optimize quality (alternative to runtime)\n            \"wallclock-limit\": 100,  # max duration to run the optimization (in seconds)\n            \"cs\": cs,  # configuration space\n            \"deterministic\": True,\n            # Uses pynisher to limit memory and runtime\n            # Alternatively, you can also disable this.\n            # Then you should handle runtime and memory yourself in the TA\n            \"limit_resources\": False,\n            \"cutoff\": 30,  # runtime limit for target algorithm\n            \"memory_limit\": 3072,  # adapt this to reasonable value for your hardware\n        }\n    )\n\n    # Max budget for hyperband can be anything. Here, we set it to maximum no. of epochs to train the MLP for\n    max_epochs = 50\n\n    # Intensifier parameters\n    intensifier_kwargs = {\"initial_budget\": 5, \"max_budget\": max_epochs, \"eta\": 3}\n\n    # To optimize, we pass the function to the SMAC-object\n    smac = SMAC4MF(\n        scenario=scenario,\n        rng=np.random.RandomState(42),\n        tae_runner=mlp_from_cfg,\n        intensifier_kwargs=intensifier_kwargs,\n    )\n\n    tae = smac.get_tae_runner()\n\n    # Example call of the function with default values\n    # It returns: Status, Cost, Runtime, Additional Infos\n    def_value = tae.run(config=cs.get_default_configuration(), budget=max_epochs, seed=0)[1]\n\n    print(\"Value for default configuration: %.4f\" % def_value)\n\n    # Start optimization\n    try:\n        incumbent = smac.optimize()\n    finally:\n        incumbent = smac.solver.incumbent\n\n    inc_value = tae.run(config=incumbent, budget=max_epochs, seed=0)[1]\n\n    print(\"Optimized Value: %.4f\" % inc_value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}